# 数据包导入
import pandas as pd
import numpy as np
import os
import sklearn
import scipy
import math

from sklearn.metrics import recall_score, f1_score, roc_curve, auc, roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import VotingClassifier

from sklearn.metrics import recall_score, f1_score, roc_curve, auc, roc_auc_score
from sklearn.model_selection import RepeatedKFold

#导入按病例预后数据文件
FilePath = 'D:/Radiomics/ICH and pneumonia/AI5/'
data = pd.read_excel(os.path.join(FilePath,'output', 'LASSO variables.xlsx'))
y = pd.DataFrame()
x_ = pd.DataFrame()
y = data['SAP']
x_ = data.iloc[:,2:] #选取自变量
x = x_

#定义开方函数
def sqrt(m):
    value = math.sqrt(m)
    return value
#定义95%置信区间计算公式
def interval(n1,n2):
    value = 1.96 * sqrt((n1 * (1 - n1))/n2)
    #value = value.astype(np.float64)
    return value
def interval_range(n1,n2):
    range_string = str('('+"%.3f" % (n1 - interval(n1,n2))+','+"%.3f" % (n1 + interval(n1,n2))+')')
    return range_string
    
#定义灵敏度、特异度
def sensitivity_metric(gt, pred): 
    pred[pred>0.5]=1
    pred[pred<1]=0
    confusion = confusion_matrix(gt,pred)
    TP = confusion[1, 1]
    TN = confusion[0, 0]
    FP = confusion[0, 1]
    FN = confusion[1, 0]
    sensitivity = TP / float(TP+FN)
    return sensitivity

#    print('准确度:',(TP+TN)/float(TP+TN+FP+FN))
#    print('灵敏度:',TP / float(TP+FN))
#    print('特异度:',TN / float(TN+FP)) 
def specificity_metric(gt, pred): 
    pred[pred>0.5]=1
    pred[pred<1]=0
    confusion = confusion_matrix(gt,pred)
    TP = confusion[1, 1]
    TN = confusion[0, 0]
    FP = confusion[0, 1]
    FN = confusion[1, 0]
    specificity = TN / float(TN+FP)
    return specificity
    
assdata = pd.DataFrame()
#p次k折交叉验证
p = 3
k = 5
rkf = RepeatedKFold(n_splits = k, n_repeats = p)
acc_svm = []
sen_svm = []
f1_svm = []
auc_svm = []
loop_num = 0
for train_index, test_index in rkf.split(x):
    loop_num = loop_num + 1
    x_train = x.iloc[train_index]
    y_train = y.iloc[train_index]
    x_test = x.iloc[test_index]
    y_test = y.iloc[test_index]
    #学习集、验证集计数
    n_train = len(y_train)
    n_test = len(y_test)
    
    
    #拟合Logistic回归模型
    model_log = LogisticRegression(max_iter=5000)
    model_log.fit(x_train,y_train)
    
    #对测试集做预测
    acc_log = model_log.score(x_test, y_test)
    y_probs = model_log.predict_proba(x_test)
    #print(y_probs) #显示预测两个分类的变量的各自概率
    y_pre_log = model_log.predict(x_test)#机器分类结果
    y_array_log = y_test.values#dataframe转化为array格式
    sen_log = sensitivity_metric(y_array_log, y_pre_log)
    spe_log = specificity_metric(y_array_log, y_pre_log)
    fpr_log, tpr_log, thresholds_log = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_log = auc(fpr_log, tpr_log)
    interval_acc_value = interval(acc_log, n_test)
    interval_acc_log = interval_range(acc_log, n_test)
    interval_sen_value = interval(sen_log, n_test)
    interval_sen_log = interval_range(sen_log, n_test)
    interval_spe_value = interval(spe_log, n_test)
    interval_spe_log = interval_range(spe_log, n_test)
    interval_auc_value = interval(auc_log, n_test)
    interval_auc_log = interval_range(auc_log, n_test)
    
    assdata_dict = {'loop_num': loop_num,
                    'alg_name':'logistic',
                    'accuracy': "%.3f" % acc_log,
                    'accuracy_interval': interval_acc_log,
                    'sensitivity_value': "%.3f" % sen_log,
                    'sensitivity_interval': interval_sen_log,
                    'specificity_value': "%.3f" % spe_log,
                    'specificity_interval': interval_spe_log,
                    'auc_value': "%.3f" % auc_log,
                    'auc_interval': interval_auc_log
                   }
    assdata = assdata.append(assdata_dict, ignore_index=True)
    
    
    #拟合朴素贝叶斯模型
    model_gnb = GaussianNB()
    model_gnb.fit(x_train,y_train)
    #对测试集做预测
    acc_gnb = model_gnb.score(x_test, y_test)
    y_probs = model_gnb.predict_proba(x_test)
    #print(y_probs) #显示预测两个分类的变量的各自概率
    y_pre_gnb = model_gnb.predict(x_test)#机器分类结果
    y_array_gnb = y_test.values#dataframe转化为array格式
    sen_gnb = sensitivity_metric(y_array_gnb, y_pre_gnb)
    spe_gnb = specificity_metric(y_array_gnb, y_pre_gnb)
    
    fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_gnb = auc(fpr_gnb, tpr_gnb)
    interval_acc_value = interval(acc_gnb, n_test)
    interval_acc_gnb = interval_range(acc_gnb, n_test)
    interval_sen_value = interval(sen_gnb, n_test)
    interval_sen_gnb = interval_range(sen_gnb, n_test)
    interval_spe_value = interval(spe_gnb, n_test)
    interval_spe_gnb = interval_range(spe_gnb, n_test)
    interval_auc_value = interval(auc_gnb, n_test)
    interval_auc_gnb = interval_range(auc_gnb, n_test)

    assdata_dict = {'loop_num': loop_num,
                    'alg_name':'GNB',
                    'accuracy': "%.3f" % acc_gnb,
                    'accuracy_interval': interval_acc_gnb,
                    'sensitivity_value': "%.3f" % sen_gnb,
                    'sensitivity_interval': interval_sen_gnb,
                    'specificity_value': "%.3f" % spe_gnb,
                    'specificity_interval': interval_spe_gnb,
                    'auc_value': "%.3f" % auc_gnb,
                    'auc_interval': interval_auc_gnb
                   }
    assdata = assdata.append(assdata_dict, ignore_index=True)
    
    
    #随机森林
    #网格调参
    #主要调参对象是n_estimators决策树数目(最佳的弱学习器迭代次数)
    params1 = {'n_estimators':range(10,71,10)}
    grid1 = GridSearchCV(estimator = RandomForestClassifier(min_samples_split=100,
                                                            min_samples_leaf=20, max_depth=8, max_features='sqrt', random_state=10),
                         param_grid =params1, scoring='roc_auc', cv=5)
    grid1.fit(x_train, y_train)
    n_estimators = grid1.best_params_['n_estimators']
    #对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索
    params2 = {'max_depth':range(3,14,2), 'min_samples_split':range(50,201,20)}
    grid2 = GridSearchCV(estimator = RandomForestClassifier(n_estimators = grid1.best_params_['n_estimators'],
                                                            min_samples_leaf = 20, max_features='sqrt', oob_score=True,random_state=10),
                         param_grid = params2, scoring='roc_auc', cv=5)
    grid2.fit(x_train, y_train)
    max_depth = grid2.best_params_['max_depth']
    min_samples_split = grid2.best_params_['min_samples_split']
    #再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参
    params3 = {'min_samples_split':range(80,150,20), 'min_samples_leaf':range(10,60,10)}
    grid3 = GridSearchCV(estimator = RandomForestClassifier(n_estimators = grid1.best_params_['n_estimators'],
                                                            max_depth = grid2.best_params_['max_depth'],
                                                            min_samples_split = grid2.best_params_['min_samples_split'],
                                                            max_features ='sqrt' ,oob_score=True, random_state=10),
                         param_grid = params3, scoring='roc_auc', cv=5)
    grid3.fit(x_train, y_train)
    min_samples_split = grid3.best_params_['min_samples_split']
    min_samples_leaf = grid3.best_params_['min_samples_leaf']
    #最后我们再对最大特征数max_features做调参:
    params4 = {'max_features':range(3,11,2)}
    grid4 = GridSearchCV(estimator = RandomForestClassifier(n_estimators = grid1.best_params_['n_estimators'],
                                                            max_depth = grid2.best_params_['max_depth'],
                                                            min_samples_split = grid2.best_params_['min_samples_split'],
                                                            min_samples_leaf = grid3.best_params_['min_samples_leaf'],
                                                            oob_score=True, random_state=10),
                         param_grid = params4, scoring='roc_auc', cv=5)
    grid4.fit(x_train, y_train)
    max_features = grid4.best_params_['max_features']
    #确认定义模型
    model_rf = RandomForestClassifier(n_estimators = grid1.best_params_['n_estimators'],
                                      max_depth = grid2.best_params_['max_depth'],
                                      min_samples_split = grid2.best_params_['min_samples_split'],
                                      min_samples_leaf = grid3.best_params_['min_samples_leaf'], 
                                      max_features = grid4.best_params_['max_features'],
                                      class_weight=None, 
                                      criterion='gini', oob_score = True, bootstrap=True)
    #拟合模型
    model_rf.fit(x_train, y_train)
    
    #对测试集做预测
    acc_rf = model_rf.score(x_test, y_test)
    y_probs = model_rf.predict_proba(x_test)
    #print(y_probs) #显示预测两个分类的变量的各自概率
    y_pre_rf = model_rf.predict(x_test)#机器分类结果
    y_array_rf = y_test.values#dataframe转化为array格式
    sen_rf = sensitivity_metric(y_array_rf, y_pre_rf)
    spe_rf = specificity_metric(y_array_rf, y_pre_rf)
    
    fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_rf = auc(fpr_rf, tpr_rf)
    interval_acc_value = interval(acc_rf, n_test)
    interval_acc_rf = interval_range(acc_rf, n_test)
    interval_sen_value = interval(sen_rf, n_test)
    interval_sen_rf = interval_range(sen_rf, n_test)
    interval_spe_value = interval(spe_rf, n_test)
    interval_spe_rf = interval_range(spe_rf, n_test)
    interval_auc_value = interval(auc_rf, n_test)
    interval_auc_rf = interval_range(auc_rf, n_test)
    
    assdata_dict = {'loop_num': loop_num,
                    'alg_name':'RF',
                    'accuracy': "%.3f" % acc_rf,
                    'accuracy_interval': interval_acc_rf,
                    'sensitivity_value': "%.3f" % sen_rf,
                    'sensitivity_interval': interval_sen_rf,
                    'specificity_value': "%.3f" % spe_rf,
                    'specificity_interval': interval_spe_rf,
                    'auc_value': "%.3f" % auc_rf,
                    'auc_interval': interval_auc_rf
                   }
    assdata = assdata.append(assdata_dict, ignore_index=True)
    
    
    #KNN调参
    ks = np.arange(1,22)
    ps = np.arange(1,7)
    param_grid = dict(n_neighbors = ks, p = ps)
    #网格化搜索（均等权重）
    grid_uni = GridSearchCV(KNeighborsClassifier(algorithm='auto', weights='uniform'), 
                            param_grid = param_grid, cv = 5)
    #auto为自动选择算法，cv为k折交叉验证（None默认三折验证），param_grid为需要最优化的参数的取值
    grid_uni.fit(x_train, y_train)
    k = grid_uni.best_params_['n_neighbors']
    p = grid_uni.best_params_['p']
    #设置knn分类器
    model_knn_uni = KNeighborsClassifier(n_neighbors=k, p=p, algorithm='auto', weights='uniform')
    #进行训练
    model_knn_uni.fit(x_train, y_train)
    #使用训练好的knn进行数据预测
    y_probs = model_knn_uni.predict_proba(x_test)
    y_pre = model_knn_uni.predict(x_test)
    y_array = y_test.values
    recall_knn_uni = recall_score(y_test, y_pre, average='weighted')
    f1_knn_uni = f1_score(y_test, y_pre, average='weighted', sample_weight=None)
    fpr_knn_uni, tpr_knn_uni, thresholds_knn_uni = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_knn_uni = auc(fpr_knn_uni, tpr_knn_uni)
    #不均权重训练
    grid_dis = GridSearchCV(KNeighborsClassifier(algorithm='auto', weights='distance'), 
                            param_grid = param_grid, cv = 5)
    grid_dis.fit(x_train, y_train)
    k = grid_dis.best_params_['n_neighbors']
    p = grid_dis.best_params_['p']
    model_knn_dis = KNeighborsClassifier(n_neighbors=k, p=p, algorithm='auto', weights='distance')
    model_knn_dis.fit(x_train, y_train)
    y_probs = model_knn_dis.predict_proba(x_test)
    y_pre = model_knn_dis.predict(x_test)
    y_array = y_test.values
    recall_knn_dis = recall_score(y_test, y_pre, average='weighted')
    f1_knn_dis = f1_score(y_test, y_pre, average='weighted', sample_weight=None)
    fpr_knn_dis, tpr_knn_dis, thresholds_knn_dis = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_knn_dis = auc(fpr_knn_dis, tpr_knn_dis)
    #选择KNN距离权重方式
    if auc_knn_uni>auc_knn_dis:
        model_knn=model_knn_uni
    elif auc_knn_uni<=auc_knn_dis:
        model_knn=model_knn_dis
    
    #对测试集做预测
    acc_knn = model_knn.score(x_test, y_test)
    y_probs = model_knn.predict_proba(x_test)
    #print(y_probs) #显示预测两个分类的变量的各自概率
    y_pre_knn = model_knn.predict(x_test)#机器分类结果
    y_array_knn = y_test.values#dataframe转化为array格式
    sen_knn = sensitivity_metric(y_array_knn, y_pre_knn)
    spe_knn = specificity_metric(y_array_knn, y_pre_knn)
    
    fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_knn = auc(fpr_knn, tpr_knn)
    interval_acc_value = interval(acc_knn, n_test)
    interval_acc_knn = interval_range(acc_knn, n_test)
    interval_sen_value = interval(sen_knn, n_test)
    interval_sen_knn = interval_range(sen_knn, n_test)
    interval_spe_value = interval(spe_knn, n_test)
    interval_spe_knn = interval_range(spe_knn, n_test)
    interval_auc_value = interval(auc_knn, n_test)
    interval_auc_knn = interval_range(auc_knn, n_test)
    
    assdata_dict = {'loop_num': loop_num,
                    'alg_name':'KNN',
                    'accuracy': "%.3f" % acc_knn,
                    'accuracy_interval': interval_acc_knn,
                    'sensitivity_value': "%.3f" % sen_knn,
                    'sensitivity_interval': interval_sen_knn,
                    'specificity_value': "%.3f" % spe_knn,
                    'specificity_interval': interval_spe_knn,
                    'auc_value': "%.3f" % auc_knn,
                    'auc_interval': interval_auc_knn
                   }
    assdata = assdata.append(assdata_dict, ignore_index=True)
    
    
    #SVM
    gammas = np.logspace(-4,1,50,base = 2)
    Cs = np.logspace(-1,3,10,base = 2)
    param_grid = dict(gamma = gammas, C = Cs)
    grid = GridSearchCV(svm.SVC(kernel = 'rbf'), param_grid = param_grid, cv = 5)
    #kernel为核函数，cv为k折交叉验证（None默认三折验证），param_grid为需要最优化的参数的取值
    grid.fit(x_train, y_train)
    gamma = grid.best_params_['gamma']
    C = grid.best_params_['C']
    model_svm = svm.SVC(kernel = 'rbf', gamma = gamma, C = C, probability = True)
    model_svm.fit(x_train, y_train)
    
    #对测试集做预测
    acc_svm = model_svm.score(x_test, y_test)
    y_probs = model_svm.predict_proba(x_test)
    #print(y_probs) #显示预测两个分类的变量的各自概率
    y_pre_svm = model_svm.predict(x_test)#机器分类结果
    y_array_svm = y_test.values#dataframe转化为array格式
    sen_svm = sensitivity_metric(y_array_svm, y_pre_svm)
    spe_svm = specificity_metric(y_array_svm, y_pre_svm)

    fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_svm = auc(fpr_svm, tpr_svm)
    interval_acc_value = interval(acc_svm, n_test)
    interval_acc_svm = interval_range(acc_svm, n_test)
    interval_sen_value = interval(sen_svm, n_test)
    interval_sen_svm = interval_range(sen_svm, n_test)
    interval_spe_value = interval(spe_svm, n_test)
    interval_spe_svm = interval_range(spe_svm, n_test)
    interval_auc_value = interval(auc_svm, n_test)
    interval_auc_svm = interval_range(auc_svm, n_test)

    assdata_dict = {'loop_num': loop_num,
                    'alg_name':'SVM',
                    'accuracy': "%.3f" % acc_svm,
                    'accuracy_interval': interval_acc_svm,
                    'sensitivity_value': "%.3f" % sen_svm,
                    'sensitivity_interval': interval_sen_svm,
                    'specificity_value': "%.3f" % spe_svm,
                    'specificity_interval': interval_spe_svm,
                   'auc_value': "%.3f" % auc_svm,
                    'auc_interval': interval_auc_svm
                   }
    assdata = assdata.append(assdata_dict, ignore_index=True)
    
    
    #XGBoosting
    params1 = { 'max_depth':range(3,11,1), 
               'min_child_weight':range(11,31,2)}
    grid1 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,n_estimators=140,gamma=0,subsample=0.8,
                                                   colsample_bytree=0.8,nthread=4,scale_pos_weight=1
                                                  ), 
                         param_grid = params1,scoring='roc_auc',n_jobs=4,cv=5
                        )
    grid1.fit(x_train, y_train)
    max_depth = grid1.best_params_['max_depth']
    min_child_weight = grid1.best_params_['min_child_weight']
    #111
    params2 = {'min_child_weight':range(min_child_weight-2,min_child_weight+2,1)}
    grid2 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=max_depth,
                                                   min_child_weight=min_child_weight,gamma=0,subsample=0.8,
                                                   colsample_bytree=0.8,nthread=4,scale_pos_weight=1
                                                  ), 
                         param_grid = params2,scoring='roc_auc',n_jobs=4,cv=5
                        )
    grid2.fit(x_train, y_train)
    min_child_weight = grid2.best_params_['min_child_weight']
    #111
    params3 = {'gamma':[i/10.0 for i in range(0,5)]}
    grid3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=max_depth,
                                                   min_child_weight=min_child_weight,subsample=0.8,
                                                   colsample_bytree=0.8,nthread=4,scale_pos_weight=1
                                                  ), 
                         param_grid = params3,scoring='roc_auc',n_jobs=4,cv=5
                        )
    grid3.fit(x_train, y_train)
    gamma = grid3.best_params_['gamma']
    #111
    params4 = {'subsample':[i/100.0 for i in range(60,100,5)], 
               'colsample_bytree':[i/100.0 for i in range(60,100,5)]}
    grid4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=max_depth,
                                                   min_child_weight=min_child_weight,gamma=gamma,nthread=4,
                                                   scale_pos_weight=1
                                                  ), 
                         param_grid = params4, scoring='roc_auc',n_jobs=4,cv=5
                        )
    grid4.fit(x_train, y_train)
    subsample = grid4.best_params_['subsample']
    colsample_bytree = grid4.best_params_['colsample_bytree']
    #111
    params5 = {'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]}
    grid5 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=max_depth,
                                                   min_child_weight=min_child_weight,gamma=gamma,
                                                   subsample=subsample,colsample_bytree=colsample_bytree,
                                                   nthread=4,
                                                   scale_pos_weight=1
                                                  ), 
                         param_grid = params5,scoring='roc_auc',n_jobs=4,cv=5
                        )
    grid5.fit(x_train, y_train)
    reg_alpha = grid5.best_params_['reg_alpha']
    #拟合模型
    model_xgb = XGBClassifier(learning_rate =0.1,n_estimators=140,max_depth=max_depth,
                              min_child_weight=min_child_weight,gamma=gamma,subsample=subsample,
                              colsample_bytree=colsample_bytree,reg_alpha=reg_alpha,nthread=4,
                              scale_pos_weight=1
                             )
    model_xgb.fit(x_train, y_train)
    
    #对测试集做预测
    acc_xgb = model_xgb.score(x_test, y_test)
    y_probs = model_xgb.predict_proba(x_test)
    #print(y_probs) #显示预测两个分类的变量的各自概率
    y_pre_xgb = model_xgb.predict(x_test)#机器分类结果
    y_array_xgb = y_test.values#dataframe转化为array格式
    sen_xgb = sensitivity_metric(y_array_xgb, y_pre_xgb)
    spe_xgb = specificity_metric(y_array_xgb, y_pre_xgb)
    
    fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_xgb = auc(fpr_xgb, tpr_xgb)
    interval_acc_value = interval(acc_xgb, n_test)
    interval_acc_xgb = interval_range(acc_xgb, n_test)
    interval_sen_value = interval(sen_xgb, n_test)
    interval_sen_xgb = interval_range(sen_xgb, n_test)
    interval_spe_value = interval(spe_xgb, n_test)
    interval_spe_xgb = interval_range(spe_xgb, n_test)
    interval_auc_value = interval(auc_xgb, n_test)
    interval_auc_xgb = interval_range(auc_xgb, n_test)

    assdata_dict = {'loop_num': loop_num,
                    'alg_name':'XGB',
                    'accuracy': "%.3f" % acc_xgb,
                    'accuracy_interval': interval_acc_xgb,
                    'sensitivity_value': "%.3f" % sen_xgb,
                    'sensitivity_interval': interval_sen_xgb,
                    'specificity_value': "%.3f" % spe_xgb,
                    'specificity_interval': interval_spe_xgb,
                    'auc_value': "%.3f" % auc_xgb,
                    'auc_interval': interval_auc_xgb
                   }
    assdata = assdata.append(assdata_dict, ignore_index=True)
    
    
    #投票模型
    model_vot = VotingClassifier(estimators = [
                                ('model_log',model_log),
                                ('model_gnb',model_gnb),
                                ('model_rf',model_rf),
                                ('model_knn',model_knn),
                                ('model_svm',model_svm),
                                ('model_xgb',model_xgb)], voting='soft')
    model_vot.fit(x_train, y_train)
    #对测试集做预测
    acc_vot = model_vot.score(x_test, y_test)
    y_probs = model_vot.predict_proba(x_test)
    #print(y_probs) #显示预测两个分类的变量的各自概率
    y_pre_vot = model_vot.predict(x_test)#机器分类结果
    y_array_vot = y_test.values#dataframe转化为array格式
    sen_vot = sensitivity_metric(y_array_vot, y_pre_vot)
    spe_vot = specificity_metric(y_array_vot, y_pre_vot)
    
    fpr_vot, tpr_vot, thresholds_vot = roc_curve(y_test, y_probs[:,1], pos_label =1)
    auc_vot = auc(fpr_vot, tpr_vot)
    interval_acc_value = interval(acc_vot, n_test)
    interval_acc_vot = interval_range(acc_vot, n_test)
    interval_sen_value = interval(sen_vot, n_test)
    interval_sen_vot = interval_range(sen_vot, n_test)
    interval_spe_value = interval(spe_vot, n_test)
    interval_spe_vot = interval_range(spe_vot, n_test)
    interval_auc_value = interval(auc_vot, n_test)
    interval_auc_vot = interval_range(auc_vot, n_test)
    assdata_dict = {'loop_num': loop_num,
                    'alg_name':'ESVM',
                    'accuracy': "%.3f" % acc_vot,
                    'accuracy_interval': interval_acc_vot,
                    'sensitivity_value': "%.3f" % sen_vot,
                    'sensitivity_interval': interval_sen_vot,
                    'specificity_value': "%.3f" % spe_vot,
                    'specificity_interval': interval_spe_vot,
                    'auc_value': "%.3f" % auc_vot,
                    'auc_interval': interval_auc_vot
                   }
    assdata = assdata.append(assdata_dict, ignore_index=True)
    
import os
def mkdir(path):
    folder = os.path.exists(path)
    if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹
        os.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径
        print('已创建新文件夹')
    else:
        print('已存在该文件夹')
mkdir(os.path.join(FilePath,'output','cross'))             #调用函数

print(assdata)
assdata.to_excel(os.path.join(FilePath,'output','cross','ass_data.xlsx'))






#After processing this excel file, another new python project should be set for heatmap plotting.
import pandas as pd
import numpy as np
import os
import sklearn
import scipy
import matplotlib.pyplot as plt
from matplotlib import gridspec
import seaborn as sns

#导入按病例预后数据文件，显示数据基本信息
FilePath = 'D:/Radiomics/ICH and pneumonia/AI5/output/'
df_log = pd.read_excel(os.path.join(FilePath,'cm&hm', 'cross_log.xlsx'))
df_gnb = pd.read_excel(os.path.join(FilePath,'cm&hm', 'cross_gnb.xlsx'))
df_rf = pd.read_excel(os.path.join(FilePath,'cm&hm', 'cross_rf.xlsx'))
df_svm = pd.read_excel(os.path.join(FilePath,'cm&hm', 'cross_svm.xlsx'))
df_knn = pd.read_excel(os.path.join(FilePath,'cm&hm', 'cross_knn.xlsx'))
df_xgb = pd.read_excel(os.path.join(FilePath,'cm&hm', 'cross_xgb.xlsx'))
df_vot = pd.read_excel(os.path.join(FilePath,'cm&hm', 'cross_vot.xlsx'))
print('数据特征：')
print(df_log.info())
print(df_gnb.info())
print(df_rf.info())
print(df_knn.info())
print(df_svm.info())
print(df_xgb.info())
print(df_vot.info())

df_log.index = ['Split 1','Split 2','Split 3']
df_gnb.index = ['Split 1','Split 2','Split 3']
df_rf.index = ['Split 1','Split 2','Split 3']
df_knn.index = ['Split 1','Split 2','Split 3']
df_svm.index = ['Split 1','Split 2','Split 3']
df_xgb.index = ['Split 1','Split 2','Split 3']
df_vot.index = ['Split 1','Split 2','Split 3']

fig = plt.figure(figsize = (5,16),dpi = 200) #fig, ((ax11,ax12),(ax21,ax22)) = plt.subplots(nrows=2, ncols=2, figsize = (12,6))
#cmap = sns.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)
plt.subplots_adjust(wspace=0.3 ,hspace=0.3)

titlesize = 14
lablesize = 10
annotsize = 10
cmmax = 100
cmmin = 0
hmmax = 0.85
hmmin = 0.65

#Log回归

ax_log_hm = plt.subplot(7,1,1)
sns.heatmap(df_log, ax = ax_log_hm, 
            vmax=hmmax, vmin=hmmin, #颜色的最大值和最小值
            linewidths = 0.05, linecolor= 'white', #块间距、块间颜色
            cmap=plt.cm.Blues, center=None, robust=False, #块色参数（xx，颜色整体偏移，鲁棒性）
            annot=True, fmt='.3f', annot_kws={'size':annotsize,'weight':'bold', 'color':'white'}, #填充数值参数（是否、小数点后位数、字体及色）
           )
ax_log_hm.set_title('Logistics Regression',fontsize=titlesize, ha="center")
ax_log_hm.set_xticklabels([])#设置x轴图例为空值
ax_log_hm.set_xlabel('')
ax_log_hm.set_yticklabels(['Split 1','Split 2','Split 3'], fontsize=lablesize, rotation=30) 
ax_log_hm.set_ylabel('')

#GNB
ax_gnb_hm = plt.subplot(7,1,2)
sns.heatmap(df_gnb, ax = ax_gnb_hm, 
            vmax=hmmax, vmin=hmmin, #颜色的最大值和最小值
            linewidths = 0.05, linecolor= 'white', #块间距、块间颜色
            cmap=plt.cm.Blues, center=None, robust=False, #块色参数（xx，颜色整体偏移，鲁棒性）
            annot=True, fmt='.3f', annot_kws={'size':annotsize,'weight':'bold', 'color':'white'}, #填充数值参数（是否、小数点后位数、字体及色）
           )
ax_gnb_hm.set_title('Gussian Naive Bayes',fontsize=titlesize, ha="center")
ax_gnb_hm.set_xticklabels([])
ax_gnb_hm.set_xlabel('') 
ax_gnb_hm.set_yticklabels(['Split 1','Split 2','Split 3'], fontsize=lablesize, rotation=30) 
ax_gnb_hm.set_ylabel('')

#随机森林
ax_rf_hm = plt.subplot(7,1,3)
sns.heatmap(df_rf, ax = ax_rf_hm, 
            vmax=hmmax, vmin=hmmin, #颜色的最大值和最小值
            linewidths = 0.05, linecolor= 'white', #块间距、块间颜色
            cmap=plt.cm.Blues, center=None, robust=False, #块色参数（xx，颜色整体偏移，鲁棒性）
            annot=True, fmt='.3f', annot_kws={'size':annotsize,'weight':'bold', 'color':'white'}, #填充数值参数（是否、小数点后位数、字体及色）
           )
ax_rf_hm.set_title('Random Forest',fontsize=titlesize, ha="center")
ax_rf_hm.set_xticklabels([])
ax_rf_hm.set_xlabel('') 
ax_rf_hm.set_yticklabels(['Split 1','Split 2','Split 3'], fontsize=lablesize, rotation=30) 
ax_rf_hm.set_ylabel('')

#K近邻
ax_knn_hm = plt.subplot(7,1,4)
sns.heatmap(df_knn, ax = ax_knn_hm, 
            vmax=hmmax, vmin=hmmin, #颜色的最大值和最小值
            linewidths = 0.05, linecolor= 'white', #块间距、块间颜色
            cmap=plt.cm.Blues, center=None, robust=False, #块色参数（xx，颜色整体偏移，鲁棒性）
            annot=True, fmt='.3f', annot_kws={'size':annotsize,'weight':'bold', 'color':'white'}, #填充数值参数（是否、小数点后位数、字体及色）
           )
ax_knn_hm.set_title('K-Nearest Neighbor',fontsize=titlesize, ha="center")
ax_knn_hm.set_xticklabels([])
ax_knn_hm.set_xlabel('') 
ax_knn_hm.set_yticklabels(['Split 1','Split 2','Split 3'], fontsize=lablesize, rotation=30) 
ax_knn_hm.set_ylabel('')

#支持向量机
ax_svm_hm = plt.subplot(7,1,5)
sns.heatmap(df_svm, ax = ax_svm_hm, 
            vmax=hmmax, vmin=hmmin, #颜色的最大值和最小值
            linewidths = 0.05, linecolor= 'white', #块间距、块间颜色
            cmap=plt.cm.Blues, center=None, robust=False, #块色参数（xx，颜色整体偏移，鲁棒性）
            annot=True, fmt='.3f', annot_kws={'size':annotsize,'weight':'bold', 'color':'white'}, #填充数值参数（是否、小数点后位数、字体及色）
           )
ax_svm_hm.set_title('Support Vector Machine',fontsize=titlesize, ha="center")
ax_svm_hm.set_xticklabels([])
ax_svm_hm.set_xlabel('') 
ax_svm_hm.set_yticklabels(['Split 1','Split 2','Split 3'], fontsize=lablesize, rotation=30) 
ax_svm_hm.set_ylabel('')

#XGBoosting
ax_xgb_hm = plt.subplot(7,1,6)
sns.heatmap(df_xgb, ax = ax_xgb_hm, 
            vmax=hmmax, vmin=hmmin, #颜色的最大值和最小值
            linewidths = 0.05, linecolor= 'white', #块间距、块间颜色
            cmap=plt.cm.Blues, center=None, robust=False, #块色参数（xx，颜色整体偏移，鲁棒性）
            annot=True, fmt='.3f', annot_kws={'size':annotsize,'weight':'bold', 'color':'white'}, #填充数值参数（是否、小数点后位数、字体及色）
           )
ax_xgb_hm.set_title('EXtreme Gradient Boosting',fontsize=titlesize, ha="center")
ax_xgb_hm.set_xticklabels([])
ax_xgb_hm.set_xlabel('') 
ax_xgb_hm.set_yticklabels(['Split 1','Split 2','Split 3'], fontsize=lablesize, rotation=30) 
ax_xgb_hm.set_ylabel('')

#Voting
ax_vot_hm = plt.subplot(7,1,7)
sns.heatmap(df_vot, ax = ax_vot_hm, 
            vmax=hmmax, vmin=hmmin, #颜色的最大值和最小值
            linewidths = 0.05, linecolor= 'white', #块间距、块间颜色
            cmap=plt.cm.Blues, center=None, robust=False, #块色参数（xx，颜色整体偏移，鲁棒性）
            annot=True, fmt='.3f', annot_kws={'size':annotsize,'weight':'bold', 'color':'white'}, #填充数值参数（是否、小数点后位数、字体及色）
           )
ax_vot_hm.set_title('Ensemble Soft Voting Model',fontsize=titlesize, ha="center")
ax_vot_hm.set_xticklabels(['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5'], fontsize=lablesize)
ax_vot_hm.set_xlabel('') 
ax_vot_hm.set_yticklabels(['Split 1','Split 2','Split 3'], fontsize=lablesize, rotation=30) 
ax_vot_hm.set_ylabel('')

plt.savefig(os.path.join(FilePath,'cm&hm','cm&hm.png'))
fig.show
